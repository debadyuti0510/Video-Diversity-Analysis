{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2079a7fe5b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and pre-processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.to(device)\n",
    "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_model.to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_brackets = [\"0-2\", \"3-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"more than 70\"]\n",
    "age_texts = [f\"A person in the {c} age group\" for c in age_brackets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_and_zs(sample):\n",
    "\n",
    "    # Age prediction\n",
    "    inputs = processor(text=age_texts, images=sample[\"image\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    age_pred = logits_per_image.argmax(dim=1) # we can take the argmax\n",
    "    \n",
    "    sample[\"zs_age_clip\"] = [int(gp) for gp in age_pred]\n",
    "    \n",
    "    # Store embeddings - dim 512\n",
    "    sample[\"proj_embeddings\"] = outputs.image_embeds\n",
    "\n",
    "    #  # Store embeddings - output of encoder, not projection - dim 768\n",
    "    # inputs = processor(images=sample[\"image\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    # outputs = vision_model(**inputs)\n",
    "    # sample[\"vm_embeddings\"] = outputs.pooler_output\n",
    "\n",
    "    # # Reduce the age by 2\n",
    "    # sample[\"age\"] = [age - 2 for age in sample[\"age\"]] # Since classes 0 and 1 have been deleted\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_ds = datasets.load_dataset('HuggingFaceM4/FairFace', '1.25', split='train', verification_mode=\"no_checks\")\n",
    "train_ds = train_ds.shuffle(seed=42) #.filter(lambda sample: sample[\"age\"] not in {0, 1}) # Filter out the first two classes\n",
    "train_ds = train_ds.map(get_embedding_and_zs, batched = True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86744"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'age', 'gender', 'race', 'service_test', 'zs_age_clip', 'proj_embeddings'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_ds[0][\"proj_embeddings\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(train_ds[0][\"vm_embeddings\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dir for embeddings and zs\n",
    "if not os.path.exists(\"embeddings/\"):\n",
    "    os.mkdir(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection embeddings\n",
    "np.save(\"embeddings/train_project_embeddings.npy\", np.array(train_ds[\"proj_embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoder embeddings\n",
    "# np.save(\"embeddings/train_encoder_embeddings.npy\", np.array(train_ds[\"vm_embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save age ground truth\n",
    "np.save(\"embeddings/train_age.npy\", np.array(train_ds[\"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save age ground truth\n",
    "np.save(\"embeddings/train_zs_age.npy\", np.array(train_ds[\"zs_age_clip\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fb162e737640ffa4d05dc04d9b56c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load training data\n",
    "# Load validation data and test on this\n",
    "test_valid_ds = datasets.load_dataset('HuggingFaceM4/FairFace', '1.25', split=\"validation\", verification_mode=\"no_checks\")\n",
    "test_valid_ds = test_valid_ds.shuffle(seed=42)# .filter(lambda sample: sample[\"age\"] not in {0, 1}) # Filter out the first two classes\n",
    "valid_ds = test_valid_ds.select([i for i in range(6_000)]) # Take only first 6_000 images'\n",
    "test_ds = test_valid_ds.select([i for i in range(6_000, len(test_valid_ds))])\n",
    "valid_ds = valid_ds.map(get_embedding_and_zs, batched = True, batch_size=16)\n",
    "test_ds = test_ds.map(get_embedding_and_zs, batched = True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection embeddings\n",
    "np.save(\"embeddings/val_project_embeddings.npy\", np.array(valid_ds[\"proj_embeddings\"]))\n",
    "# Save encoder embeddings\n",
    "# np.save(\"embeddings/val_encoder_embeddings.npy\", np.array(valid_ds[\"vm_embeddings\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/val_age.npy\", np.array(valid_ds[\"age\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/val_zs_age.npy\", np.array(valid_ds[\"zs_age_clip\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection embeddings\n",
    "np.save(\"embeddings/test_project_embeddings.npy\", np.array(test_ds[\"proj_embeddings\"]))\n",
    "# Save encoder embeddings\n",
    "# np.save(\"embeddings/test_encoder_embeddings.npy\", np.array(test_ds[\"vm_embeddings\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/test_age.npy\", np.array(test_ds[\"age\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/test_zs_age.npy\", np.array(test_ds[\"zs_age_clip\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
