{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import datasets\n",
    "import joblib\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25eeecb37f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Debadyuti\\.conda\\envs\\dissertation-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model and pre-processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.to(device)\n",
    "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vision_model.to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_brackets = [\"10-19\", \"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"more than 70\"]\n",
    "age_texts = [f\"A person in the {c} age group\" for c in age_brackets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_and_zs(sample):\n",
    "\n",
    "    # Age prediction\n",
    "    inputs = processor(text=age_texts, images=sample[\"image\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    age_pred = logits_per_image.argmax(dim=1) # we can take the argmax\n",
    "    \n",
    "    sample[\"zs_age_clip\"] = [int(gp) for gp in age_pred]\n",
    "    \n",
    "    # Store embeddings - dim 512\n",
    "    sample[\"proj_embeddings\"] = outputs.image_embeds\n",
    "\n",
    "     # Store embeddings - output of encoder, not projection - dim 768\n",
    "    inputs = processor(images=sample[\"image\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = vision_model(**inputs)\n",
    "    sample[\"vm_embeddings\"] = outputs.pooler_output\n",
    "\n",
    "    # Reduce the age by 2\n",
    "    sample[\"age\"] = [age - 2 for age in sample[\"age\"]] # Since classes 0 and 1 have been deleted\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061ca95ee732460e8f3349bd0a951e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load training data\n",
    "train_ds = datasets.load_dataset('HuggingFaceM4/FairFace', '1.25', split='train', verification_mode=\"no_checks\")\n",
    "train_ds = train_ds.shuffle(seed=42).filter(lambda sample: sample[\"age\"] not in {0, 1}) # Filter out the first two classes\n",
    "train_ds = train_ds.map(get_embedding_and_zs, batched = True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'age', 'gender', 'race', 'service_test', 'zs_age_clip', 'proj_embeddings', 'vm_embeddings'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_ds[0][\"proj_embeddings\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_ds[0][\"vm_embeddings\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dir for embeddings and zs\n",
    "if not os.path.exists(\"embeddings/\"):\n",
    "    os.mkdir(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection embeddings\n",
    "np.save(\"embeddings/train_project_embeddings.npy\", np.array(train_ds[\"proj_embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoder embeddings\n",
    "np.save(\"embeddings/train_encoder_embeddings.npy\", np.array(train_ds[\"vm_embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save age ground truth\n",
    "np.save(\"embeddings/train_age.npy\", np.array(train_ds[\"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save age ground truth\n",
    "np.save(\"embeddings/train_zs_age.npy\", np.array(train_ds[\"zs_age_clip\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23b3ac99b2a4865aa85d5c7dc28c689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4d7fc192bb43d3bfb900b95c5d11e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load training data\n",
    "# Load validation data and test on this\n",
    "test_valid_ds = datasets.load_dataset('HuggingFaceM4/FairFace', '1.25', split=\"validation\", verification_mode=\"no_checks\")\n",
    "test_valid_ds = test_valid_ds.shuffle(seed=42).filter(lambda sample: sample[\"age\"] not in {0, 1}) # Filter out the first two classes\n",
    "valid_ds = test_valid_ds.select([i for i in range(6_000)]) # Take only first 6_000 images'\n",
    "test_ds = test_valid_ds.select([i for i in range(6_000, len(test_valid_ds))])\n",
    "valid_ds = valid_ds.map(get_embedding_and_zs, batched = True, batch_size=16)\n",
    "test_ds = test_ds.map(get_embedding_and_zs, batched = True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection embeddings\n",
    "np.save(\"embeddings/val_project_embeddings.npy\", np.array(valid_ds[\"proj_embeddings\"]))\n",
    "# Save encoder embeddings\n",
    "np.save(\"embeddings/val_encoder_embeddings.npy\", np.array(valid_ds[\"vm_embeddings\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/val_age.npy\", np.array(valid_ds[\"age\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/val_zs_age.npy\", np.array(valid_ds[\"zs_age_clip\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projection embeddings\n",
    "np.save(\"embeddings/test_project_embeddings.npy\", np.array(test_ds[\"proj_embeddings\"]))\n",
    "# Save encoder embeddings\n",
    "np.save(\"embeddings/test_encoder_embeddings.npy\", np.array(test_ds[\"vm_embeddings\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/test_age.npy\", np.array(test_ds[\"age\"]))\n",
    "# Save age ground truth\n",
    "np.save(\"embeddings/test_zs_age.npy\", np.array(test_ds[\"zs_age_clip\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
